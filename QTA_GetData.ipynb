{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc16ed4",
   "metadata": {},
   "source": [
    "# QTA - Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632dd266",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c5ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983fad03",
   "metadata": {},
   "source": [
    "Data is stored in .XML files which we need to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for parsing data\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21cf3bc",
   "metadata": {},
   "source": [
    " Additionally, we employ multiprocessing for faster execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fdcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for multiprocessing\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Check core count\n",
    "num_cores = mp.cpu_count()\n",
    "print(num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9799ab8",
   "metadata": {},
   "source": [
    "## Load Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b7a72",
   "metadata": {},
   "source": [
    "We define the data to load. We could set a specific sample to only load a certain amount of articles for fast code testing. However, ultimately we want to load all the articles in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06cfb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set corpus to the folder of files you want to use\n",
    "corpus = '/home/ec2-user/SageMaker/data/QTA_-_titles/'\n",
    "\n",
    "# Read in files\n",
    "input_files = os.listdir(corpus)\n",
    "\n",
    "# Select the number of articles to sample\n",
    "sample_input_files = input_files# [0:50000] # len(input_files)\n",
    "    \n",
    "print(\"Currently sampling\", len(sample_input_files), \"documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a16a77",
   "metadata": {},
   "source": [
    "We define the function to parse the .XML tree. We also include the function for strip html text. (These functions were part of the example script from TDM Studio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to strip html tags from text portion\n",
    "def strip_html_tags(text):\n",
    "    stripped = BeautifulSoup(text).get_text().replace('\\n', ' ').replace('\\\\', '').strip()\n",
    "    return stripped\n",
    "\n",
    "\n",
    "# Retrieve metadata from XML document\n",
    "def getxmlcontent(corpus, file, strip_html=True):\n",
    "    try:\n",
    "        tree = etree.parse(corpus + file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        if root.find('.//GOID') is not None:\n",
    "            goid = root.find('.//GOID').text\n",
    "        else:\n",
    "            goid = None\n",
    "\n",
    "        if root.find('.//Title') is not None:\n",
    "            title = root.find('.//Title').text\n",
    "        else:\n",
    "            title = None\n",
    "\n",
    "        if root.find('.//NumericDate') is not None:\n",
    "            date = root.find('.//NumericDate').text\n",
    "        else:\n",
    "            date = None\n",
    "            \n",
    "        if root.find('.//PublisherName') is not None:\n",
    "            publisher = root.find('.//PublisherName').text\n",
    "        else:\n",
    "            publisher = None\n",
    "\n",
    "        if root.find('.//FullText') is not None:\n",
    "            text = root.find('.//FullText').text\n",
    "\n",
    "        elif root.find('.//HiddenText') is not None:\n",
    "            text = root.find('.//HiddenText').text\n",
    "\n",
    "        elif root.find('.//Text') is not None:\n",
    "            text = root.find('.//Text').text\n",
    "\n",
    "        else:\n",
    "            text = None\n",
    "\n",
    "        # Strip html from text portion\n",
    "        if text is not None and strip_html == True:\n",
    "            text = strip_html_tags(text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while parsing file {file}: {e}\")\n",
    "    \n",
    "    return goid, title, date, publisher, text\n",
    "\n",
    "\n",
    "# Function to make lists out of parsed data--on single document scale for multiprocessing\n",
    "def make_lists(file):\n",
    "    \n",
    "    goid, title, date, publisher, text = getxmlcontent(corpus, file, strip_html=True)\n",
    "    \n",
    "    return goid, publisher, title, text, date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490955d",
   "metadata": {},
   "source": [
    "We test the function on a single document before we apply it through multiprocessing to all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c893222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function on single document\n",
    "make_lists(input_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "try:\n",
    "    p = Pool(processes=num_cores-1)\n",
    "    processed_lists = p.map(make_lists, sample_input_files)\n",
    "except Exception as e:\n",
    "    print(f\"Error in processing document: {e}\")\n",
    "finally:\n",
    "    p.close()\n",
    "    \n",
    "df = pd.DataFrame(processed_lists, columns=['GOID', 'Publisher', 'Title', 'Text', 'Date'])\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2424d5",
   "metadata": {},
   "source": [
    "Finally, we have a dataframe that includes everything that we read in (i.e. GOID, Publisher, Title, Text, Date) for each of our articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b11878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort for reproducibility\n",
    "df_sorted = df.sort_values(by = [\"Date\", \"Title\"])\n",
    "df = df_sorted.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataframe\n",
    "print(df.iloc[0])\n",
    "print()\n",
    "\n",
    "# View full example\n",
    "print(df.iloc[0][\"Title\"])\n",
    "print()\n",
    "print(print(df.iloc[0][\"Text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec462ba1",
   "metadata": {},
   "source": [
    "## Filter data based on company name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9829c6c",
   "metadata": {},
   "source": [
    "The goal is to filter articles down in multiple iterations, because each simple iteration makes the next filter less computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc982b",
   "metadata": {},
   "source": [
    "First, we filter out the articles that contain a company name in the title and the articles that contain a company name in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d3fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = pd.read_csv(\"Companies.csv\", encoding=\"cp1252\")\n",
    "company_df = company_df[company_df[\"Companies\"] != \"Target\"]\n",
    "company_df = company_df[company_df[\"Companies\"] != \"Visa\"]\n",
    "\n",
    "comp_df = company_df[\"Companies\"].to_list()\n",
    "pattern_comp = re.compile(r'\\b(?:' + '|'.join(re.escape(company) for company in comp_df) + r')\\b')\n",
    "\n",
    "\n",
    "def contains_company_name(text, pattern):\n",
    "    if text:        \n",
    "        return bool(re.search(pattern, text))\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90304b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'\\b(?:Exxon\\ Mobil|Wal\\-Mart\\ Stores|General\\ Motors|Ford\\ Motor|General\\ Electric|Citigroup|Enron|Intl\\.\\ Business\\ Machines|AT\\&T|Verizon\\ Communications|Altria\\ Group|Chase\\ Manhattan\\ Corp\\.|Bank\\ of\\ America\\ Corp\\.|SBC\\ Communications|Boeing|Texaco|Duke\\ Energy|Kroger|Hewlett\\-Packard|ChevronTexaco|State\\ Farm\\ Insurance\\ Cos|American\\ Intl\\.\\ Group|Home\\ Depot|Morgan\\ Stanley|Merrill\\ Lynch|Fannie\\ Mae|Compaq\\ Computer|Lucent\\ Technologies|Sears\\ Roebuck|Merck|Procter\\ \\&\\ Gamble|MCI\\ WorldCom|TIAA\\-CREF|Motorola|McKesson|Kmart\\ Holding|Albertson\\'s|Marathon\\ Oil|Berkshire\\ Hathaway|Intel|Goldman\\ Sachs\\ Group|J\\.C\\.\\ Penney|Conoco|Costco\\ Wholesale|Safeway|MetLife|Dell|Ingram\\ Micro|Freddie\\ Mac|Cardinal\\ Health|United\\ Parcel\\ Service|Pfizer|Dynegy|CenterPoint\\ Energy|DuPont|Johnson\\ \\&\\ Johnson|Delphi|Allstate|Aquila|International\\ Paper|Wells\\ Fargo|Aetna|United\\ Technologies|Lehman\\ Brothers\\ Hldgs\\.|BellSouth|Walt\\ Disney|ConAgra\\ Foods|Lockheed\\ Martin|Bank\\ One\\ Corp\\.|Honeywell\\ Intl\\.|Tosco|Wachovia\\ Corp\\.|American\\ Express|Sprint|Southern|Alcoa|Dow\\ Chemical|Microsoft|Prudential\\ Financial|FleetBoston\\ Financial|PG\\&E\\ Corp\\.|AutoNation|Georgia\\-Pacific|TXU|El\\ Paso|New\\ York\\ Life\\ Insurance|Bristol\\-Myers\\ Squibb|ConocoPhillips|Walgreen|UnitedHealth\\ Group|Loews|Coca\\-Cola|PepsiCo|Tech\\ Data|Sara\\ Lee|Supervalu|AMR|Caterpillar|CVS|Viacom|Cigna|Bergen\\ Brunswig|UAL|Sysco|Electronic\\ Data\\ Systems|Cisco\\ Systems|Lowe\\'s|Xerox|Federated\\ Dept\\.\\ Stores|Raytheon|FedEx|Monsanto|TRW|Johnson\\ Controls|Northwestern\\ Mutual|IBP|3M|Columbia/HCA\\ Healthcare|Qwest\\ Communications|Liberty\\ Mutual\\ Ins\\.\\ Group|Weyerhaeuser|Delta\\ Air\\ Lines|Washington\\ Mutual|Sun\\ Microsystems|Emerson\\ Electric|Nationwide|Coca\\-Cola\\ Enterprises|Hartford\\ Financial\\ Services|Rite\\ Aid|Valero\\ Energy|Publix\\ Super\\ Markets|Occidental\\ Petroleum|May\\ Dept\\.\\ Stores|Fleming|Goodyear\\ Tire\\ \\&\\ Rubber|Ultramar\\ Diamond\\ Shamrock|McDonald\\'s|Solectron|Lear|Eastman\\ Kodak|Kimberly\\-Clark|Wyeth|Abbott\\ Laboratories|Winn\\-Dixie\\ Stores|American\\ Electric\\ Power|Gap|Halliburton|Deere|Textron|Arrow\\ Electronics|Archer\\ Daniels\\ Midland|Dana|Sunoco|Circuit\\ City\\ Stores|Best\\ Buy|Waste\\ Management|Amerada\\ Hess|Anheuser\\-Busch|Farmland\\ Industries|Household\\ International|Union\\ Pacific|Texas\\ Instruments|Edison\\ International|AmerisourceBergen|Xcel\\ Energy|Office\\ Depot|Williams|PacifiCare\\ Health\\ Sys\\.|Northwest\\ Airlines|Tenet\\ Healthcare|Toys\\ `R`\\ Us|Mass\\.\\ Mutual\\ Life\\ Ins\\.|American\\ General|Fluor|Eli\\ Lilly|Manpower|Staples|Humana|General\\ Dynamics|Whirlpool|Bear\\ Stearns|Marsh\\ \\&\\ McLennan|Oracle|Limited\\ Brands|Marriott\\ International|Entergy|Gillette|Illinois\\ Tool\\ Works|U\\.S\\.\\ Bancorp|Continental\\ Airlines|Schering\\-Plough|AFLAC|Gateway|TJX|Applied\\ Materials|Consolidated\\ Edison|UnumProvident|H\\.J\\.\\ Heinz|Computer\\ Sciences|Colgate\\-Palmolive|US\\ Airways\\ Group|Dominion\\ Resources|WellPoint\\ Health\\ Networks|Ingersoll\\-Rand|Burlington\\ No\\.\\ Santa\\ Fe|Unocal|Avnet|Health\\ Net|National\\ City\\ Corp\\.|CMS\\ Energy|Nike|Eaton|Guardian\\ Life\\ of\\ America|Principal\\ Financial|EMC|Smurfit\\-Stone\\ Container|Dillard\\'s|Wellpoint|PPG\\ Industries|SunTrust\\ Banks|St\\.\\ Paul\\ Travelers\\ Cos\\.|CHS|USAA|CSX|KeyCorp|Navistar\\ International|Cinergy|Genuine\\ Parts|SCI\\ Systems|Conseco|Northrop\\ Grumman|Comcast|Ashland|Reynolds\\ American|Apple\\ Computer|Pepsi\\ Bottling|Paccar|Avista|MBNA|OfficeMax|PNC\\ Financial\\ Services|American\\ Standard|John\\ Hancock\\ Financial\\ Svcs\\.|Exelon|Bank\\ of\\ New\\ York\\ Co\\.|Aon|Micron\\ Technology|Premcor|Crown\\ Holdings|Corning|Aramark|Chubb|Masco|Rockwell\\ Automation|Tyson\\ Foods|Baxter\\ International|Sempra\\ Energy|Charles\\ Schwab|NorthWestern|Safeco|Yum\\ Brands|FPL\\ Group|Alltel|FirstEnergy|Adams\\ Resources\\ \\&\\ Engy\\.|Kellogg|Newell\\ Rubbermaid|America\\ Online|Unisys|Rohm\\ \\&\\ Haas|Lincoln\\ National|Public\\ Service\\ Enterprise\\ Group|Express\\ Scripts|Progressive|General\\ Mills|AES|ONEOK|Plains\\ Resources|Cummins|Saks|Union\\ Carbide|Seagate\\ Technology|Automatic\\ Data\\ Proc\\.|Campbell\\ Soup|Gannett|CNF|CIT\\ Group|Norfolk\\ Southern|Omnicom\\ Group|Kohl\\'s|NiSource|Computer\\ Assoc\\.\\ Intl\\.|Science\\ Applications\\ Intl\\.|Sonic\\ Automotive|Federal\\-Mogul|Mellon\\ Financial\\ Corp\\.|ServiceMaster|NCR|Centex|Providian\\ Financial|State\\ St\\.\\ Corp\\.|Northeast\\ Utilities|Owens\\-Illinois|R\\.R\\.\\ Donnelley\\ \\&\\ Sons|Dean\\ Foods|VF|Avon\\ Products|Nextel\\ Communications|Allied\\ Waste\\ Industries|First\\ Data|Anadarko\\ Petroleum|PPL|Southwest\\ Airlines|Interpublic\\ Group|DTE\\ Energy|Mattel|Litton\\ Industries|Engelhard|Goodrich|Nordstrom|Air\\ Products\\ \\&\\ Chem\\.|Fortune\\ Brands|Ikon\\ Office\\ Solutions|Capital\\ One\\ Financial|Dover|3Com|Parker\\ Hannifin|Clear\\ Channel\\ Communications|BB\\&T\\ Corp\\.|Ryder\\ System|Eastman\\ Chemical|Baker\\ Hughes|Graybar\\ Electric|Sherwin\\-Williams|GPU|Meritor\\ Automotive|Smithfield\\ Foods|KeySpan|Tesoro|Tribune|TransMontaigne|Praxair|Quaker\\ Oats|Conectiv|Medtronic|W\\.W\\.\\ Grainger|Owens\\ Corning|LTV|BJ\\'s\\ Wholesale\\ Club|Pacific\\ Life|Caesars\\ Entertainment|United\\ Auto\\ Group|ITT\\ Industries|Peter\\ Kiewit\\ Sons\\'|Crestline\\ Capital|RadioShack|American\\ Family\\ Ins\\.\\ Grp\\.|Dole\\ Food|Sodexho\\ Marriott\\ Services|Quantum|Lennar|Foot\\ Locker|Cendant|Willamette\\ Industries|Advanced\\ Micro\\ Devices|Murphy\\ Oil|IAC/Interactive|AK\\ Steel\\ Holding|Nucor|Black\\ \\&\\ Decker|Dollar\\ General|Golden\\ State\\ Bancorp|Niagara\\ Mohawk\\ Holdings|Phelps\\ Dodge|Pitney\\ Bowes|Brunswick|Kelly\\ Services|AutoZone|Cooper\\ Industries|Caremark\\ Rx|Cablevision\\ Systems|Starwood\\ Hotels\\ \\&\\ Rsrts\\.|Flowers\\ Industries|Mead|Estee\\ Lauder|Temple\\-Inland|McGraw\\-Hill|Leggett\\ \\&\\ Platt|Fifth\\ Third\\ Bancorp|Mutual\\ of\\ Omaha\\ Ins\\.|Maytag|Avis\\ Rent\\ A\\ Car|Brink\\'s|Hershey\\ Foods|Bethlehem\\ Steel|HealthSouth|USA\\ Education|Pulte\\ Homes|True\\ Value|Jones\\ Apparel\\ Group|Kerr\\-McGee|Progress\\ Energy|Autoliv|Oxford\\ Health\\ Plans|Encompass\\ Services|Barnes\\ \\&\\ Noble|Comerica|Clorox|Lyondell\\ Chemical|Longs\\ Drug\\ Stores|Nash\\ Finch|Allegheny\\ Energy|Ames\\ Dept\\.\\ Stores|Merisel|Foster\\ Wheeler|Golden\\ West\\ Financial|ShopKo\\ Stores|United\\ Stationers|KB\\ Home|FMC|Sanmina\\-SCI|SouthTrust\\ Corp\\.|Avery\\ Dennison|York\\ International|Thermo\\ Electron|Wesco\\ International|Constellation\\ Energy|Comdisco\\ Holding|Ameren|CDW|Regions\\ Financial|Lexmark\\ International|Hasbro|American\\ Financial\\ Grp\\.|USG|Danaher|Pathmark\\ Stores|Spherion|AmSouth\\ Bancorp\\.|Kinder\\ Morgan|Westvaco|Fleetwood\\ Enterprises|Administaff|Darden\\ Restaurants|Pinnacle\\ West\\ Capital|MicroAge|Hormel\\ Foods|Ball|D\\.R\\.\\ Horton|Amgen|Becton\\ Dickinson|Yellow\\ Roadway|Group\\ 1\\ Automotive|Reliance\\ Group\\ Holdings|Jabil\\ Circuit|Tenneco\\ Automotive|Northern\\ Trust\\ Corp\\.|Trans\\ World\\ Airlines|Interstate\\ Bakeries|Anixter\\ International|Cox\\ Communications|Owens\\ \\&\\ Minor|New\\ York\\ Times|Cooper\\ Tire\\ \\&\\ Rubber|Harrah\\'s\\ Entertainment|Emcor\\ Group|Hilton\\ Hotels|Puget\\ Energy|SCANA|Quest\\ Diagnostics|Jacobs\\ Engineering\\ Grp\\.|Thrivent\\ Financial\\ for\\ Lutherans|Universal|Tellabs|Wisconsin\\ Energy|Steelcase|Hughes\\ Supply|OGE\\ Energy|Allmerica\\ Financial|Borders\\ Group|ADC\\ Telecommunications|Western\\ Gas\\ Resources|Big\\ Lots|Airborne|Armstrong\\ Holdings|Mohawk\\ Industries|Charter\\ Communications|Lennox\\ International|Summit\\ Bancorp|Jefferson\\-Pilot|MGM\\ Mirage|Lutheran\\ Brotherhood|Pennzoil\\-Quaker\\ State|Knight\\-Ridder|Qualcomm|Chevron|J\\.P\\.\\ Morgan\\ Chase\\ \\&\\ Co\\.|American\\ International\\ Group|CVS\\ Caremark|International\\ Business\\ Machines|WellPoint|State\\ Farm\\ Insurance\\ Cos\\.|Medco\\ Health\\ Solutions|Sears\\ Holdings|International\\ Assets\\ Holding|Kraft\\ Foods|Apple|Sprint\\ Nextel|Liberty\\ Mutual\\ Insurance\\ Group|Honeywell\\ International|News\\ Corp\\.|HCA|Hess|Time\\ Warner|Enterprise\\ GP\\ Holdings|Massachusetts\\ Mutual\\ Life\\ Insurance|Philip\\ Morris\\ International|Travelers\\ Cos\\.|Amazon\\.com|Google|Macy\\'s|DirecTV\\ Group|GMAC|PNC\\ Financial\\ Services\\ Group|Plains\\ All\\ American\\ Pipeline|Time\\ Warner\\ Cable|United\\ Services\\ Automobile\\ Association|L\\-3\\ Communications|Freeport\\-McMoRan\\ Copper\\ \\&\\ Gold|Burlington\\ Northern\\ Santa\\ Fe|Coventry\\ Health\\ Care|Toys\\ \"R\"\\ Us|CBS|National\\ Oilwell\\ Varco|Community\\ Health\\ Systems|KBR|DISH\\ Network|TRW\\ Automotive\\ Holdings|Jacobs\\ Engineering\\ Group|World\\ Fuel\\ Services|Oneok|Liberty\\ Global|United\\ States\\ Steel|ITT|SAIC|Land\\ O\\'Lakes|Liberty\\ Media|Mosaic|Thermo\\ Fisher\\ Scientific|Unum\\ Group|Guardian\\ Life\\ Ins\\.\\ Co\\.\\ of\\ America|Starbucks|Penske\\ Automotive\\ Group|Energy\\ Future\\ Holdings|Great\\ Atlantic\\ \\&\\ Pacific\\ Tea|State\\ Street\\ Corp\\.|Pepco\\ Holdings|URS|GameStop|Genworth\\ Financial|XTO\\ Energy|Devon\\ Energy|NRG\\ Energy|Automatic\\ Data\\ Processing|eBay|Assurant|Apache|Air\\ Products\\ \\&\\ Chemicals|Bank\\ of\\ New\\ York\\ Mellon\\ Corp\\.|Smith\\ International|Republic\\ Services|Boston\\ Scientific|Whole\\ Foods\\ Market|Discover\\ Financial\\ Services|Ameriprise\\ Financial|Icahn\\ Enterprises|Huntsman|Synnex|Newmont\\ Mining|Chesapeake\\ Energy|C\\.H\\.\\ Robinson\\ Worldwide|Integrys\\ Energy\\ Group|Western\\ Digital|Family\\ Dollar\\ Stores|Estée\\ Lauder|Shaw\\ Group|Bed\\ Bath\\ \\&\\ Beyond|Ross\\ Stores|Pilgrim\\'s\\ Pride|Hertz\\ Global\\ Holdings|Reinsurance\\ Group\\ of\\ America|CarMax|Gilead\\ Sciences|Precision\\ Castparts|Commercial\\ Metals|WellCare\\ Health\\ Plans|Western\\ Refining|Stryker|Visteon|AGCO|Calpine|Henry\\ Schein|Affiliated\\ Computer\\ Services|Yahoo|American\\ Family\\ Insurance\\ Group|Peabody\\ Energy|Omnicare|AECOM\\ Technology|Symantec|SLM|DaVita|MeadWestvaco|Virgin\\ Media|First\\ American\\ Corp\\.|Enbridge\\ Energy\\ Partners|Ecolab|Fidelity\\ National\\ Financial|Global\\ Partners|UGI|Harris|CC\\ Media\\ Holdings|Dr\\ Pepper\\ Snapple\\ Group|SunGard\\ Data\\ Systems|CH2M\\ Hill|Pantry|Domtar|Oshkosh|Energy\\ Transfer\\ Equity|Advance\\ Auto\\ Parts|PetSmart|Reliance\\ Steel\\ \\&\\ Aluminum|Hershey|YRC\\ Worldwide|Dollar\\ Tree|Dana\\ Holding|Cameron\\ International|Nash\\-Finch|Terex|Universal\\ Health\\ Services|Amerigroup|Jarden|Tutor\\ Perini|Mutual\\ of\\ Omaha\\ Insurance|Avis\\ Budget\\ Group|MasterCard|Mylan|Western\\ Union|Celanese|Telephone\\ \\&\\ Data\\ Systems|Polo\\ Ralph\\ Lauren|Auto\\-Owners\\ Insurance|Core\\-Mark\\ Holding|Western\\ \\&\\ Southern\\ Financial\\ Group|CenturyTel|Atmos\\ Energy|Universal\\ American|SPX|O\\'Reilly\\ Automotive|Harley\\-Davidson|Holly|EOG\\ Resources|Spectra\\ Energy|Starwood\\ Hotels\\ \\&\\ Resorts|TravelCenters\\ of\\ America|BlackRock|Laboratory\\ Corp\\.\\ of\\ America|Health\\ Management\\ Associates|NYSE\\ Euronext|St\\.\\ Jude\\ Medical|Tenneco|Consol\\ Energy|ArvinMeritor|Lubrizol|Alliant\\ Techsystems|Washington\\ Post|Las\\ Vegas\\ Sands|Genzyme|Allergan|Broadcom|Agilent\\ Technologies|Rockwell\\ Collins|W\\.R\\.\\ Berkley|PepsiAmericas|Dick\\'s\\ Sporting\\ Goods|FMC\\ Technologies|NII\\ Holdings|General\\ Cable|Biogen\\ Idec|AbitibiBowater|Flowserve|Airgas|Kindred\\ Healthcare|American\\ Financial\\ Group|Spectrum\\ Group\\ International|CA|Con\\-way|Erie\\ Insurance\\ Group|Casey\\'s\\ General\\ Stores|Centene|Sealed\\ Air|Frontier\\ Oil|Scana|Live\\ Nation\\ Entertainment|Fiserv|Host\\ Hotels\\ \\&\\ Resorts|H\\&R\\ Block|Electronic\\ Arts|Franklin\\ Resources|MDU\\ Resources\\ Group|CB\\ Richard\\ Ellis\\ Group|Blockbuster|Walmart|CVS\\ Health|Alphabet|Walgreens\\ Boots\\ Alliance|JPMorgan\\ Chase|Phillips\\ 66|Bank\\ of\\ America|Marathon\\ Petroleum|Anthem|Dell\\ Technologies|DuPont\\ de\\ Nemours|State\\ Farm\\ Insurance|IBM|Albertsons|HP|Facebook|Energy\\ Transfer|AIG|HCA\\ Healthcare|American\\ Airlines\\ Group|United\\ Continental\\ Holdings|TIAA|Enterprise\\ Products\\ Partners|Plains\\ GP\\ Holdings|AbbVie|Hewlett\\ Packard\\ Enterprise|Twenty\\-First\\ Century\\ Fox|Travelers|INTL\\ FCStone|PBF\\ Energy|Kraft\\ Heinz|Mondelez\\ International|DXC\\ Technology|US\\ Foods\\ Holding|CenturyLink|Jabil|ManpowerGroup|Aflac|Tesla|CBRE\\ Group|AECOM|Bank\\ of\\ New\\ York\\ Mellon|Molina\\ Healthcare|Freeport\\-McMoRan|Synchrony\\ Financial|HollyFrontier|Performance\\ Food\\ Group|NGL\\ Energy\\ Partners|XPO\\ Logistics|PG\\&E|NextEra\\ Energy|Jones\\ Lang\\ LaSalle|WestRock|Cognizant\\ Technology\\ Solutions|Netflix|PayPal\\ Holdings|Celgene|Mastercard|Booking\\ Holdings|Parker\\-Hannifin|Qurate\\ Retail|Arconic|Stanley\\ Black\\ \\&\\ Decker|Biogen|Dominion\\ Energy|ADP|salesforce\\.com|L\\ Brands|Newell\\ Brands|Murphy\\ USA|LKQ|Steel\\ Dynamics|Lithia\\ Motors|MGM\\ Resorts\\ International|Nvidia|Farmers\\ Insurance\\ Exchange|Expedia\\ Group|Quanta\\ Services|Lam\\ Research|L3\\ Technologies|Molson\\ Coors\\ Brewing|Discovery|BorgWarner|Targa\\ Resources|Ally\\ Financial|IQVIA\\ Holdings|Delek\\ US\\ Holdings|United\\ Natural\\ Foods|Leidos\\ Holdings|PulteGroup|DCP\\ Midstream|PVH|Altice\\ USA|Pioneer\\ Natural\\ Resources|Coty|Vistra\\ Energy|Adobe|Brighthouse\\ Financial|Voya\\ Financial|Hilton\\ Worldwide\\ Holdings|Veritiv|Westlake\\ Chemical|Univar|J\\.B\\.\\ Hunt\\ Transport\\ Services|Frontier\\ Communications|Jones\\ Financial\\ \\(Edward\\ Jones\\)|Eversource\\ Energy|Fidelity\\ National\\ Information\\ Services|Yum\\ China\\ Holdings|Thor\\ Industries|Alaska\\ Air\\ Group|Amphenol|WESCO\\ International|Huntington\\ Ingalls\\ Industries|Jefferies\\ Financial\\ Group|Expeditors\\ Intl\\.\\ of\\ Washington|EMCOR\\ Group|SpartanNash|United\\ Rentals|Cheniere\\ Energy|Conagra\\ Brands|Zimmer\\ Biomet\\ Holdings|Tractor\\ Supply|Berry\\ Global\\ Group|Alliance\\ Data\\ Systems|Builders\\ FirstSource|EnLink\\ Midstream|WEC\\ Energy\\ Group|JetBlue\\ Airways|A\\-Mark\\ Precious\\ Metals|Constellation\\ Brands|Activision\\ Blizzard|Raymond\\ James\\ Financial|Keurig\\ Dr\\ Pepper|American\\ Tower|J\\.M\\.\\ Smucker|Citizens\\ Financial\\ Group|Motorola\\ Solutions|Magellan\\ Health|American\\ Axle\\ \\&\\ Manufacturing|Newmont\\ Goldcorp|Spirit\\ AeroSystems\\ Holdings|Fortive|NVR|Toll\\ Brothers|Sanmina|Insight\\ Enterprises|Packaging\\ Corp\\.\\ of\\ America|Olin|Arthur\\ J\\.\\ Gallagher|MasTec|Alleghany|Asbury\\ Automotive\\ Group|Markel|Blackstone\\ Group|Hanesbrands|Wayfair|Wynn\\ Resorts|Ulta\\ Beauty|Regeneron\\ Pharmaceuticals|Burlington\\ Stores|Northern\\ Trust|Chemours|Seaboard|Ascena\\ Retail\\ Group|Cintas|M\\&T\\ Bank\\ Corp\\.|ABM\\ Industries|Beacon\\ Roofing\\ Supply|iHeartMedia|Intercontinental\\ Exchange|S\\&P\\ Global|Post\\ Holdings|Analog\\ Devices|Ralph\\ Lauren|L3Harris\\ Technologies|Booz\\ Allen\\ Hamilton|Polaris\\ Industries|Realogy\\ Holdings|HD\\ Supply\\ Holdings|Graphic\\ Packaging\\ Holding|Old\\ Republic\\ International|Intuit|NetApp|Tapestry|ON\\ Semiconductor|Ingredion|Zoetis|TreeHouse\\ Foods|Robert\\ Half\\ International|First\\ American\\ Financial|Windstream\\ Holdings|Williams\\-Sonoma|Simon\\ Property\\ Group|Navient|Levi\\ Strauss|Lehman\\ Brothers|Plains\\ All\\ Amer\\.\\ Pipeline|Countrywide\\ Financial|DIRECTV\\ Group|Enterprise\\ Products|Reliant\\ Energy|Kinder\\ Morgan\\ Energy|TEPPCO\\ Partners|Echostar\\ Communications|Burlington\\ Resources|IAC/InterActiveCorp|WPS\\ Resources|Energy\\ Transfer\\ Partners|Freescale\\ Semiconductor|Ryerson|BlueLinx\\ Holdings|Fisher\\ Scientific\\ Intl\\.|Hovnanian\\ Enterprises|Energy\\ East|Timken|Beazer\\ Homes\\ USA|Triad\\ Hospitals|Avaya|MDC\\ Holdings|Liz\\ Claiborne|Ryland\\ Group|Hexion\\ Specialty\\ Chemicals|Affiliated\\ Computer\\ Svcs\\.|Western\\ \\&\\ Southern\\ Financial|Mirant|Freeport\\-McMoRan\\ Cpr\\.\\ \\&\\ Gld|Wm\\.\\ Wrigley\\ Jr\\.|Universal\\ Health\\ Svcs\\.|Standard\\ Pacific|Marshall\\ \\&\\ Ilsley\\ Corp\\.|LandAmerica\\ Financial|Verizon|Costco|JP\\ Morgan\\ Chase|Express\\ Scripts\\ Holding|Walgreens|UPS|Lowe|HCA\\ Holdings|DirecTV|Macy|McDonald|US\\ Foods|Kohl\\'s|Hartford\\ Financial\\ Services\\ Group|Kraft\\ Foods\\ Group|Land\\ O\\'Lakes|DaVita\\ HealthCare\\ Partners|Leucadia\\ National|Liberty\\ Interactive|CST\\ Brands|Peter\\ Kiewit\\ Sons\\'|Priceline\\ Group|KKR|O\\'Reilly\\ Automotive|Casey\\'s\\ General\\ Stores|Dick\\'s\\ Sporting\\ Goods|Dillard\\'s|Level\\ 3\\ Communications|Buckeye\\ Partners|SanDisk|Lansing\\ Trade\\ Group|Expeditors\\ International\\ of\\ Washington|Jones\\ Financial|Discovery\\ Communications|Trinity\\ Industries|TEGNA|HRG\\ Group|MRC\\ Global|Calumet\\ Specialty\\ Products\\ Partners|Expedia|AGL\\ Resources|Booz\\ Allen\\ Hamilton\\ Holding|Quintiles\\ Transnational\\ Holdings|Harman\\ International\\ Industries|Essendant|ARRIS\\ Group|LifePoint\\ Health|Wyndham\\ Worldwide|McGraw\\ Hill\\ Financial|Exxon|WalMart|GM|Ford|GE|Citigroup|Enron|IBM|AT\\&T|Verizon|Altria|Chase|BofA|SBC|Boeing|Texaco|Duke|Kroger|Chevron|MS|P\\&G|GS|JC\\ Penney|Costco|UPS|J\\&J|Lehman|Disney|Coca\\ Cola|Pepsi|Cisco|Cola\\-Cola|Valero|McDonalds|Lilly|BS|Allied\\ Waste|Interpublic|JP\\ Morgan|Amazon|ToysRUs|CVS|Google|JP\\ Morgan|Bofa|PayPal|MGM\\ |Jefferies|Blackstone|Levi\\'s|Lehman|ETP|JP\\ Morgan|Macy\\'s|McDonald\\'s|Booz\\ Allen|Wal\\-Mart|Citi|Bofa|Goldman|Lehman\\ Brothers|Coca\\ Cola|J\\.P\\.\\ Morgan|Toys\\ R\\ Us|J\\.P\\.\\ Morgan|Goldman\\ Sachs)\\b',\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "title_df = df[df[\"Title\"].apply(lambda x: contains_company_name(x, pattern_comp))]\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time-start_time)\n",
    "title_df.to_csv(\"QTATITLE/filtered_df_title.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e937ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "text_df = df[df[\"Text\"].apply(lambda x: contains_company_name(x, pattern_comp))]\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time-start_time)\n",
    "text_df.to_csv(\"QTATITLE/filtered_df_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd08f9e",
   "metadata": {},
   "source": [
    "## Filter by Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfea723",
   "metadata": {},
   "source": [
    "Second, we filter out the articles that contain a keyword. Here, we distinguish again between contained in the title or contained in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7daa9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scandal_df = pd.read_csv(\"keywords.csv\", index_col=False)\n",
    "scand_df = scandal_df[\"keywords\"].to_list()\n",
    "pattern_string = '|'.join(re.escape(sca) for sca in scand_df)\n",
    "pattern_string_repl = pattern_string.replace(\"\\.\\*\", \".*\")\n",
    "pattern_scandal = re.compile(r'\\b(?:' + pattern_string_repl + r')\\b', re.IGNORECASE)\n",
    "\n",
    "\n",
    "def contains_scandal(text, pattern):\n",
    "    if text:        \n",
    "        return bool(re.search(pattern, text))\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b012db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'\\b(?:corrupt.*|deceit.*|decept.*|deceiv.*|betray.*|shame.*|scandal.*|dishonest.*|misconduct.*|fraud|illegal.*|unethic.*|violat.*|falsify.*|breach.*|leak.*|pollut.*|insecur.*|irregular.*|mismanag.*|inappropriat.*|unlaw.*|transgress.*|noncomplian.*|non\\-complian.*|ghost.*|malfeas.*|exploitat.*|discriminat.*|harass.*|misrepresent.*|embezzle.*|improper.*|espion.*|collus.*|misus.*|rigg.*|kickback.*|retaliat.*|moral\\ lapse|insider\\ trading|insider\\ dealing|ponzi\\ scheme|arrest.*|product\\ recall|privacy\\ breach|privacy\\ violation\\*|data\\ leak|intellectual\\ property\\ dispute|malpracti.*|destruct.*|unsustain.*|ESG\\ scandal|fraudulent|deceptiv.*|scamm.*|briber.*|bribe.*|extort.*|misappropriat.*|sabotag.*|deforest.*|habitat\\ destruct.*|climate\\ change\\ deni.*|tax\\ evasion|money\\ launder.*|accounting\\ scandal|whistleblow.*|sexual\\ harass.*|workplace\\ harass.*|toxic\\ culture|data\\ breach|ransomware|drug\\ recall|clinical\\ trial\\ fraud|off\\-label\\ marketing|antitrust|cartel|monopoly|litigat.*|regulatory\\ breach|cover\\-up|settlement|lawsuit|penalt.*)\\b',\n",
       "re.IGNORECASE|re.UNICODE)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_scandal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1520df",
   "metadata": {},
   "source": [
    "### \"Title\" Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee8dd6",
   "metadata": {},
   "source": [
    "This filters for scandal words contained in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ebca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_title_df = pd.read_csv(\"QTATITLE/filtered_df_title.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed15a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "title_df_scand = loaded_title_df[loaded_title_df[\"Title\"].apply(lambda x: contains_scandal(x, pattern_scandal))]\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time-start_time)\n",
    "title_df_scand.to_csv(\"QTATITLE/scand_df_title.csv\") # Execution time:  3.7135045528411865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df_scand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36f376",
   "metadata": {},
   "source": [
    "### \"Text\" Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92ca99",
   "metadata": {},
   "source": [
    "This filters for scandal words contained in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_text_df = pd.read_csv(\"QTATITLE/filtered_df_text.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "text_df_scand = loaded_text_df[loaded_text_df[\"Text\"].apply(lambda x: contains_scandal(x, pattern_scandal))]\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time-start_time)\n",
    "text_df_scand.to_csv(\"QTATITLE/scand_df_text.csv\") # Execution time:  726.7273228168488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df_scand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8932c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_text_df_scand = pd.read_csv(\"QTATITLE/scand_df_text.csv\", index_col=0)\n",
    "loaded_text_df_scand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43952f92",
   "metadata": {},
   "source": [
    "We then filter again, such that we only keep an article if the scandal is contained within +/- 15 words (excl. stopwords) around a company name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = pd.read_csv(\"Companies.csv\", encoding=\"cp1252\")\n",
    "company_df = company_df[company_df[\"Companies\"] != \"Target\"]\n",
    "company_df = company_df[company_df[\"Companies\"] != \"Visa\"]\n",
    "comp_df = company_df[\"Companies\"].to_list()\n",
    "pattern_comp = re.compile(r'\\b(?:' + '|'.join(re.escape(company) for company in comp_df) + r')\\b', re.IGNORECASE)  \n",
    "\n",
    "scandal_df = pd.read_csv(\"keywords.csv\", index_col=False)\n",
    "scand_df = scandal_df[\"keywords\"].to_list()\n",
    "pattern_string = '|'.join(re.escape(sca) for sca in scand_df)\n",
    "pattern_string_repl = pattern_string.replace(\"\\.\\*\", \".*\")\n",
    "pattern_scandal = re.compile(r'\\b(?:' + pattern_string_repl + r')\\b', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e943b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.data.path.append(\"./stopwords_en\")\n",
    "stop_words = set(stopwords.words(\"english_stopwords\"))\n",
    "\n",
    "def remove_my_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def check_keywords_context(text):\n",
    "    global keywords_counter\n",
    "    keywords_counter += 1\n",
    "    if keywords_counter%1000==0:\n",
    "        print(keywords_counter)\n",
    "    filtered_text = remove_my_stopwords(text)\n",
    "\n",
    "    for match in re.finditer(pattern_comp, filtered_text):\n",
    "        company_start, company_end = match.span()\n",
    "        company_index = filtered_text[:company_start].count(\" \")\n",
    "        \n",
    "        start_index = max(0, company_index-15)\n",
    "        end_index = min(len(filtered_text.split()), company_index+15)\n",
    "        context_words = filtered_text.split()[start_index:end_index]\n",
    "        \n",
    "        context_text = \" \".join(context_words)\n",
    "        if pattern_scandal.search(context_text):\n",
    "            return True\n",
    "    \n",
    "    return False     \n",
    "\n",
    "keywords_counter = 0\n",
    "start_time = time.time()\n",
    "text_df_scand_context = loaded_text_df_scand[loaded_text_df_scand[\"Text\"].apply(lambda x: check_keywords_context(x))]\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time-start_time)\n",
    "text_df_scand_context.to_csv(\"QTATITLE/scand_df_context_text.csv\") # Execution time:  1729.6244366168976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df_scand_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578bb35",
   "metadata": {},
   "source": [
    "## Title-Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4bec57",
   "metadata": {},
   "source": [
    "Finally we merge the previous results such that we know for each article why it was kept (Titlebased, Textbased, or both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_title_df_scand = pd.read_csv(\"QTATITLE/scand_df_title.csv\", index_col=0)\n",
    "loaded_title_df_scand = loaded_title_df_scand[[\"GOID\", \"Title\", \"Text\", \"Date\"]]\n",
    "loaded_title_df_scand[\"Titlebased\"] = 1\n",
    "loaded_title_df_scand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f37ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_text_df_scand_context = pd.read_csv(\"QTATITLE/scand_df_context_text.csv\", index_col=0)\n",
    "loaded_text_df_scand_context = loaded_text_df_scand_context[[\"GOID\", \"Title\", \"Text\", \"Date\"]]\n",
    "loaded_text_df_scand_context[\"Textbased\"] = 1\n",
    "loaded_text_df_scand_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40503102",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text_df = pd.merge(loaded_title_df_scand, loaded_text_df_scand_context, left_on=\"GOID\", right_on=\"GOID\", how=\"outer\")\n",
    "\n",
    "cleaned_title_text_df = pd.DataFrame()\n",
    "\n",
    "for col in title_text_df.columns:\n",
    "    if col.endswith(\"_x\"):\n",
    "        base_col = col[:-2]\n",
    "        if base_col + \"_y\" in title_text_df.columns:\n",
    "            cleaned_title_text_df[base_col] = title_text_df[col].combine_first(title_text_df[base_col + \"_y\"])\n",
    "        else:\n",
    "            cleaned_title_text_df[base_col] = title_text_df[col]\n",
    "    elif col.endswith(\"_y\") and col[:-2] + \"_x\" not in title_text_df.columns:\n",
    "        cleaned_title_text_df[col[:-2]] = title_text_df[col]\n",
    "    elif '_' not in col:\n",
    "        cleaned_title_text_df[col] = title_text_df[col]\n",
    "         \n",
    "cleaned_title_text_df.to_csv(\"QTATITLE/cleaned_tt_df.csv\")\n",
    "cleaned_title_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tt_df = pd.read_csv(\"QTATITLE/cleaned_tt_df.csv\", index_col=0)\n",
    "loaded_tt_df[\"Text\"] = loaded_tt_df[\"Text\"].fillna(\"\")\n",
    "loaded_tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tt_df_2 = loaded_tt_df[[\"GOID\", \"Titlebased\", \"Textbased\"]]\n",
    "loaded_tt_df_2.to_csv(\"QTATITLE/cleaned_tt_df_download.csv\")\n",
    "loaded_tt_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a080849e",
   "metadata": {},
   "source": [
    "We also shorten the text, such that we only have a list of strings containing the +/-15 words around the company names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021fec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = pd.read_csv(\"Companies.csv\", encoding=\"cp1252\")\n",
    "company_df = company_df[company_df[\"Companies\"] != \"Target\"]\n",
    "company_df = company_df[company_df[\"Companies\"] != \"Visa\"]\n",
    "comp_df = company_df[\"Companies\"].to_list()\n",
    "pattern_comp = re.compile(r'\\b(?:' + '|'.join(re.escape(company) for company in comp_df) + r')\\b', re.IGNORECASE)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.data.path.append(\"./stopwords_en\")\n",
    "stop_words = set(stopwords.words(\"english_stopwords\"))\n",
    "\n",
    "def remove_my_stopwords(text):\n",
    "    try:\n",
    "        words = text.split()\n",
    "    except:\n",
    "        print(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def filter_keywords_context(text):\n",
    "    global keywords_counter\n",
    "    keywords_counter += 1\n",
    "    if keywords_counter%1000==0:\n",
    "        print(keywords_counter)\n",
    "    filtered_text = remove_my_stopwords(text)\n",
    "    \n",
    "    text_filtered_out = []\n",
    "    for match in re.finditer(pattern_comp, filtered_text):\n",
    "        company_start, company_end = match.span()\n",
    "        company_index = filtered_text[:company_start].count(\" \")\n",
    "        \n",
    "        start_index = max(0, company_index-15)\n",
    "        end_index = min(len(filtered_text.split()), company_index+15)\n",
    "        context_words = filtered_text.split()[start_index:end_index]\n",
    "        \n",
    "        context_text = \" \".join(context_words)\n",
    "        text_filtered_out.append(context_text)\n",
    "        \n",
    "    return text_filtered_out\n",
    "            \n",
    "\n",
    "keywords_counter = 0\n",
    "start_time = time.time()\n",
    "loaded_tt_df[\"words\"] = loaded_tt_df[\"Text\"].apply(lambda x: filter_keywords_context(x))\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time-start_time)\n",
    "loaded_tt_df = loaded_tt_df[[\"GOID\", \"Title\", \"words\", \"Date\"]]\n",
    "loaded_tt_df.to_csv(\"QTATITLE/tt_df_reduced.csv\") # Execution time:  900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb082312",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loaded_tt_df = pd.read_csv(\"QTATITLE/tt_df_reduced.csv\", index_col=0)\n",
    "new_loaded_tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e53ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loaded_tt_df = pd.read_csv(\"QTATITLE/tt_df_reduced.csv\", index_col=0)\n",
    "new_loaded_tt_reduced = new_loaded_tt_df[[\"GOID\", \"words\"]]\n",
    "new_loaded_tt_reduced.to_csv(\"QTATITLE/tt_df_reduced_2.csv.xz\", index=False, compression=\"xz\")\n",
    "new_loaded_tt_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"QTATITLE/tt_df_reduced_2.csv.xz\", compression=\"xz\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a466f5",
   "metadata": {},
   "source": [
    "## Prepare Final Data Set for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e88cce2",
   "metadata": {},
   "source": [
    "Lastly, we construct our full dataset based on all the iterations that we have done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bdc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e6968",
   "metadata": {},
   "source": [
    "To do so, we merge the different datasets created before, such that we have a full dataset with indicators for all previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(\"extended.csv\")\n",
    "full_df = full_df[[\"GOID\", \"Title\", \"Date\"]]\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_contains = pd.read_csv(\"title_contains.csv\", index_col=0)\n",
    "title_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.read_csv(\"tt_df_reduced_2.csv.xz\", compression=\"xz\")\n",
    "based_on = pd.read_csv(\"cleaned_tt_df_download.csv\", index_col=0)\n",
    "\n",
    "words_based_df = pd.merge(words_df, based_on, how=\"inner\", left_on=\"GOID\", right_on=\"GOID\")\n",
    "words_based_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_1 = pd.merge(full_df, title_contains, how=\"inner\", left_on=\"GOID\", right_on=\"GOID\")\n",
    "merge_2 = pd.merge(merge_1, words_based_df, how=\"left\", left_on=\"GOID\", right_on=\"GOID\")\n",
    "merge_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba40bfc",
   "metadata": {},
   "source": [
    "We also (1) create a column that holds all companies contained in the title and a column that contains all companies contained in the text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd75f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = pd.read_csv(\"Companies.csv\", encoding=\"cp1252\")\n",
    "company_df = company_df[company_df[\"Companies\"] != \"Target\"]\n",
    "company_df = company_df[company_df[\"Companies\"] != \"Visa\"]\n",
    "\n",
    "comp_df = company_df[\"Companies\"].to_list()\n",
    "pattern_comp = re.compile(r'\\b(?:' + '|'.join(re.escape(company) for company in comp_df) + r')\\b')\n",
    "\n",
    "\n",
    "def contains_company_name(text, pattern):\n",
    "    global keywords_counter\n",
    "    keywords_counter += 1\n",
    "    if keywords_counter%10000==0:\n",
    "        print(keywords_counter)\n",
    "    if text:\n",
    "        matches = pattern.findall(text)\n",
    "        return list(set(matches if matches else None))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_time = time.time()\n",
    "keywords_counter = 0\n",
    "merge_2[\"company_in_title\"] = merge_2[\"Title\"].apply(lambda x: contains_company_name(x, pattern_comp))\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = pd.read_csv(\"Companies.csv\", encoding=\"cp1252\")\n",
    "company_df = company_df[company_df[\"Companies\"] != \"Target\"]\n",
    "company_df = company_df[company_df[\"Companies\"] != \"Visa\"]\n",
    "\n",
    "comp_df = company_df[\"Companies\"].to_list()\n",
    "pattern_comp = re.compile(r'\\b(?:' + '|'.join(re.escape(company) for company in comp_df) + r')\\b')\n",
    "\n",
    "def contains_company_name_2(text, pattern):\n",
    "    global keywords_counter\n",
    "    keywords_counter += 1\n",
    "    if keywords_counter%10000==0:\n",
    "        print(keywords_counter)\n",
    "    if text and isinstance(text, str):\n",
    "        matches = pattern.findall(text)\n",
    "        if matches:\n",
    "            return list(set(matches if matches else None))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_time = time.time()\n",
    "keywords_counter = 0\n",
    "merge_2[\"company_in_text\"] = merge_2[\"words\"].apply(lambda x: contains_company_name_2(x, pattern_comp))\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f1b8e",
   "metadata": {},
   "source": [
    "...and (2) a column that contains all scandals contained in the title and a column that contains all scandals contained in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "scandal_df = pd.read_csv(\"keywords.csv\", index_col=False)\n",
    "scand_df = scandal_df[\"keywords\"].to_list()\n",
    "pattern_string = '|'.join(\"(?P<{}>{})\".format(re.sub(r'\\W', '_', sca), re.escape(sca)) for sca in scand_df)\n",
    "pattern_string_repl = pattern_string.replace(\"\\.\\*\", \".*\")\n",
    "pattern_scandal = re.compile(r'\\b(?:' + pattern_string_repl + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    \n",
    "def contains_scandal(text, pattern):\n",
    "    global keywords_counter\n",
    "    keywords_counter += 1\n",
    "    if keywords_counter%10000==0:\n",
    "        print(keywords_counter)\n",
    "    if text:\n",
    "        matches = [name for match in pattern.finditer(text) for name, value in match.groupdict().items() if value]\n",
    "        if matches:\n",
    "            return list(set(matches if matches else None))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_time = time.time()\n",
    "keywords_counter = 0\n",
    "merge_2[\"scandal_in_title\"] = merge_2[\"Title\"].apply(lambda x: contains_scandal(x, pattern_scandal))\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c37219",
   "metadata": {},
   "outputs": [],
   "source": [
    "scandal_df = pd.read_csv(\"keywords.csv\", index_col=False)\n",
    "scand_df = scandal_df[\"keywords\"].to_list()\n",
    "pattern_string = '|'.join(\"(?P<{}>{})\".format(re.sub(r'\\W', '_', sca), re.escape(sca)) for sca in scand_df)\n",
    "pattern_string_repl = pattern_string.replace(\"\\.\\*\", \".*\")\n",
    "pattern_scandal = re.compile(r'\\b(?:' + pattern_string_repl + r')\\b', re.IGNORECASE)\n",
    "\n",
    "def contains_scandal_2(text, pattern):\n",
    "    global keywords_counter\n",
    "    keywords_counter += 1\n",
    "    if keywords_counter%10000==0:\n",
    "        print(keywords_counter)\n",
    "    if text and isinstance(text, str):\n",
    "        matches = [name for match in pattern.finditer(text) for name, value in match.groupdict().items() if value]\n",
    "        if matches:\n",
    "            return list(set(matches if matches else None))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_time = time.time()\n",
    "keywords_counter = 0\n",
    "merge_2[\"scandal_in_text\"] = merge_2[\"words\"].apply(lambda x: contains_scandal_2(x, pattern_scandal))\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08660143",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_2_unique = merge_2.drop_duplicates(subset='Title', keep=\"first\")\n",
    "merge_2_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac075708",
   "metadata": {},
   "source": [
    "## Merge in comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6e5cf",
   "metadata": {},
   "source": [
    "Finally, we merge in all data from the validation, i.e. the data from doccano and the data from ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71139865",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_comparison = pd.read_csv(\"GPT_comparing.csv\", index_col=0)\n",
    "gpt_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dabbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scandals_manual_comparison = pd.read_csv(\"scandals_manually_comparing.csv\", index_col=0)\n",
    "scandals_manual_comparison = scandals_manual_comparison.rename(columns={\"text\":\"Title\", \"Unnamed: 2\":\"A\", \"Date\":\"B\", \"label\":\"scandals_manual\",\"Comments\":\"C\"})\n",
    "scandals_manual_comparison = scandals_manual_comparison[[\"Title\", \"scandals_manual\"]]\n",
    "scandals_manual_comparison[\"scandals_manual\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e37ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_manual_comparison = pd.read_csv(\"all_manually_comparing.csv\", index_col=0)\n",
    "all_manual_comparison = all_manual_comparison .rename(columns={\"Unnamed: 4\":\"scandals_all\"})\n",
    "all_manual_comparison = all_manual_comparison [[\"Title\", \"scandals_all\"]]\n",
    "all_manual_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a1b27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_3 = pd.merge(merge_2_unique, gpt_comparison[[\"Title\", \"GPT\"]], how=\"left\", left_on=\"Title\", right_on=\"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_4 = pd.merge(merge_3, scandals_manual_comparison, how=\"left\", left_on=\"Title\", right_on=\"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_5 = pd.merge(merge_4, all_manual_comparison, how=\"left\", left_on=\"Title\", right_on=\"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf041b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_5.to_csv(\"final_data_with_count_and_annotation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d3715b",
   "metadata": {},
   "source": [
    "This merge also allows us to create various confusion matrices for title, text or both. The code can be easily adapted for a certain comparison, here we have the code for the GPT comparison and the code for a doccano comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637865bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxx = pd.merge(merge_2_unique, gpt_comparison, how=\"inner\", left_on=\"Title\", right_on=\"Title\")\n",
    "xxxxx = xxxxx[[\"Textbased\", \"GPT\", \"Titlebased\"]].fillna('N')\n",
    "xxxxx[\"Textbased\"] = xxxxx[\"Textbased\"].apply(lambda x: \"Y\" if x==1.0 else \"N\")\n",
    "xxxxx[\"Titlebased\"] = xxxxx[\"Titlebased\"].apply(lambda x: \"Y\" if x==1.0 else \"N\")\n",
    "# xxxxx[\"Textbased\"] = xxxxx[\"Textbased\"] + xxxxx[\"Titlebased\"]\n",
    "# xxxxx[\"Textbased\"] = xxxxx[\"Textbased\"].apply(lambda x: \"Y\" if x==\"YY\" or x==\"NY\" or x==\"YN\" else \"N\")\n",
    "\n",
    "cm = confusion_matrix(xxxxx[\"GPT\"], xxxxx[\"Titlebased\"], labels=['N', 'Y'])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ecbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxx = pd.merge(merge_2_unique, scandals_manual_comparison, how=\"inner\", left_on=\"Title\", right_on=\"Title\")\n",
    "xxxxx = xxxxx[[\"Textbased\", \"scandals_manual\", \"Titlebased\"]].fillna('N')\n",
    "xxxxx[\"Titlebased\"] = xxxxx[\"Titlebased\"].apply(lambda x: \"Y\" if x==1.0 else \"N\")\n",
    "#xxxxx[\"Titlebased\"] = xxxxx[\"Titlebased\"].apply(lambda x: \"Y\" if x==1.0 else \"N\")\n",
    "#xxxxx[\"Textbased\"] = xxxxx[\"Textbased\"] + xxxxx[\"Titlebased\"]\n",
    "#xxxxx[\"Textbased\"] = xxxxx[\"Textbased\"].apply(lambda x: \"Y\" if x==\"YY\" or x==\"NY\" or x==\"YN\" else \"N\")\n",
    "\n",
    "cm = confusion_matrix(xxxxx[\"scandals_manual\"], xxxxx[\"Titlebased\"], labels=['N', 'Y'])\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c624ae",
   "metadata": {},
   "source": [
    "## Load Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1a579",
   "metadata": {},
   "source": [
    "Now the data is ready for analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b64fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_data_with_count_and_annotation.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72767814",
   "metadata": {},
   "source": [
    "...or so we thought until we checked the results. We realized that there were some significant results for variations and abbreviations of company names, e.g. BofA and Bank of America both had a good amount of scandals. Thus, we are mapping the company spellings (for the most important, i.e., having the most results, ones) back to a single company name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc6d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_mapping = {\n",
    "    \"Wal-Mart\": [\"Wal-Mart Stores\", \"Walmart\"],\n",
    "    \"IBM\": [\"Intl. Business Machines\", \"International Business Machines\"],\n",
    "    \"Citi\": [\"Citigroup\"],\n",
    "    \"AIG\": [\"American Intl. Group\", \"American International Group\"],\n",
    "    \"J.P. Morgan\": [\"JP Morgan\", \"JPMorgan Chase\", \"J.P. Morgan Chase & Co.\", \"JPM\"],\n",
    "    \"MS\": [\"Morgan Stanley\", \"MS\"],\n",
    "    \"McDonald's\": [\"McDonald\", \"McDonalds\"],\n",
    "    \"Verizon\": [\"Verizon Communications\"],\n",
    "    \"Sprint\": [\"Sprint Nextel\"],\n",
    "    \"Dell\": [\"Dell Technologies\"],\n",
    "    \"Exxon\": [\"Exxon Mobil\"],\n",
    "    \"Lehman\": [\"Lehman Brothers Hldgs.\", \"Lehman Brothers\"],\n",
    "    \"BofA\": [\"Bank of America Corp.\", \"Bank of America\"],\n",
    "    \"Travelers\": [\"Travelers Cos.\"],\n",
    "    \"Cisco\": [\"Cisco Systems\"],\n",
    "    \"Chevron\": [\"ChevronTexaco\"],\n",
    "    \"Blackstone\": [\"Blackstone Group\"],\n",
    "    \"UPS\": [\"United Parcel Service\"],\n",
    "    \"Coca-Cola\": [\"Coca Cola\"],\n",
    "    \"DuPont\": [\"DuPont de Nemours\"],\n",
    "    \"CVS\": [\"CVS Health\", \"CVS Caremark\"],\n",
    "    \"Pepsi\": [\"PepsiCo\"],\n",
    "    \"SBC\": [\"SBC Communications\"],\n",
    "    \"Macy's\": [\"Macy\"],\n",
    "    \"General Motors\": [\"GM\"],\n",
    "    \"DirecTV\": [\"DirecTV Group\"],\n",
    "    \"Duke\": [\"Duke Energy\"],\n",
    "    \"American Express\": [\"AmEx\"],\n",
    "    \"MasterCard\": [\"Mastercard\"],\n",
    "    \"Walmart\": [\"Wal-Mart\", \"Wal-Mart Stores\"],\n",
    "    \"Goldman\": [\"Goldman Sachs Group\", \"GS\"],\n",
    "    \"Disney\": [\"Walt Disney\"],\n",
    "    \"Apple\": [\"Apple Computer\"],\n",
    "    \"Google\": [\"Alphabet\"],\n",
    "    \"Procter & Gamble\": [\"P&G\"],\n",
    "    \"Johnson & Johnson\": [\"J&J\"],\n",
    "    \"ToysRUs\": [\"Toys R Us\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e886e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_company_names(companies, mapping):\n",
    "    reversed_mapping = {alias: key for key, aliases in mapping.items() for alias in aliases}\n",
    "    return str([reversed_mapping.get(company, company) for company in eval(companies)])\n",
    "\n",
    "df[\"company_in_title\"] = df[\"company_in_title\"].apply(lambda x: replace_company_names(x, company_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_company_names(companies, mapping):\n",
    "    reversed_mapping = {alias: key for key, aliases in mapping.items() for alias in aliases}\n",
    "    if isinstance(companies, str):\n",
    "        return str([reversed_mapping.get(company, company) for company in eval(companies)])\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "df[\"company_in_text\"] = df[\"company_in_text\"].apply(lambda x: replace_company_names(x, company_mapping))\n",
    "df['company_in_text'] = df['company_in_text'].apply(lambda x: np.nan if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b778c",
   "metadata": {},
   "source": [
    "And with that, we really arrive at the final dataset that can be used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"FINAL_DATASET_FIX.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad49063",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df = pd.read_csv(\"final_data_with_count_and_annotation.csv\", index_col=0)\n",
    "loaded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74283765",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4b95e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
