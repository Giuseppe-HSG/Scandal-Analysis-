{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Dataset of all articles about companies (without duplicates)\n",
    "\n",
    "all_articles = pd.read_csv(\"FINAL_DATASET_FIX.csv\", index_col=0)\n",
    "all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Articles which mention a scandal in the title of the article \n",
    "scandal_title = all_articles[all_articles[\"Titlebased\"]==1.0]\n",
    "scandal_title.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scandal_title.to_csv(\"scandal_in_title.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Articles which mention a scandal in the text of the article \n",
    "scandal_text = all_articles[all_articles[\"Textbased\"]==1.0]\n",
    "scandal_text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scandal_text.to_csv(\"scandal_in_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Articles which mention a scandal in the title AND text of the article (not used in the reseearch)\n",
    "scandal_title_text = scandal_title[scandal_title[\"Textbased\"]==1.0]\n",
    "scandal_title_text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEVERITY SCORE CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severity Score for the Title Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_scandal = pd.read_csv(\"scandal_in_title.csv\", index_col=0)\n",
    "title_scandal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to a date\n",
    "title_scandal['Date'] = pd.to_datetime(title_scandal['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to count additional articles in the following 14 days\n",
    "def count_additional_articles(row, data):\n",
    "    company = row['company_in_title']\n",
    "    start_date = row['Date']\n",
    "    end_date = start_date + timedelta(days=14)\n",
    "    \n",
    "    # Filter for articles about the same company within the next month\n",
    "    mask = (title_scandal['company_in_title'] == company) & (title_scandal['Date'] > start_date) & (title_scandal['Date'] <= end_date)\n",
    "    return title_scandal[mask].shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate severity scores for each articles\n",
    "title_scandal['severity_score'] = title_scandal.apply(lambda row: count_additional_articles(row, title_scandal), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most severe articles\n",
    "title_scandal[title_scandal['severity_score'] > 5].sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_scandal.to_csv(\"title_scandal_severity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severity Score for Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_scandal = pd.read_csv(\"scandal_in_text.csv\", index_col=0)\n",
    "text_scandal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text_scandal['Date'] = pd.to_datetime(text_scandal['Date'])\n",
    "title_scandal['Date'] = pd.to_datetime(title_scandal['Date'])\n",
    "\n",
    "# Function to count additional articles in the following 14 days\n",
    "def count_additional_articles(row, text_scandal):\n",
    "    company = row['company_in_text']\n",
    "    start_date = row['Date']\n",
    "    end_date = start_date + timedelta(days=14)\n",
    "    \n",
    "    # Filter for articles about the same company within the next week\n",
    "    mask = (text_scandal['company_in_text'] == company) & (text_scandal['Date'] > start_date) & (text_scandal['Date'] <= end_date)\n",
    "    return text_scandal[mask].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the function to the scandal articles DataFrame to calculate severity scores\n",
    "title_scandal['severity_score'] = title_scandal.apply(lambda row: count_additional_articles(row, text_scandal), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_based_scandal = title_scandal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_based_scandal[text_based_scandal['severity_score'] > 10].sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_based_scandal.to_csv(\"text_scandal_severity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severity Score Based on All Company Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_articles['Date'] = pd.to_datetime(all_articles['Date'])\n",
    "title_scandal['Date'] = pd.to_datetime(title_scandal['Date'])\n",
    "\n",
    "# Function to count additional articles in the following 14 days\n",
    "def count_additional_articles(row, text_scandal):\n",
    "    company = row['company_in_title']\n",
    "    start_date = row['Date']\n",
    "    end_date = start_date + timedelta(days=14)\n",
    "    \n",
    "    # Filter for articles about the same company within the next 14 days\n",
    "    mask = (all_articles['company_in_title'] == company) & (all_articles['Date'] > start_date) & (all_articles['Date'] <= end_date)\n",
    "    return all_articles[mask].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the function to the scandal articles DataFrame to calculate severity scores\n",
    "title_scandal['severity_score'] = title_scandal.apply(lambda row: count_additional_articles(row, all_articles), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_based_scandal = title_scandal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_based_scandal[all_based_scandal['severity_score'] > 23].sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_based_scandal.to_csv(\"all_scandal_severity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot with Volume of Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Load and sort the scandal data\n",
    "all_articles = pd.read_csv('FINAL_DATASET_FIX.csv')\n",
    "scandal_articles_df = scandal_articles_df.sort_values(by='Date', ascending=True)\n",
    "scandal_articles_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the stock data (Check the excel to know what is what company)\n",
    "stock_data = pd.read_excel(\"../Stock_FINAL.xlsx\", sheet_name = 1)\n",
    "stock_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We plot for Apple in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To plot for other companies, just change the sheet number and company name in the code\n",
    "#### Check severity score of the most severe articles to check which period to plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title_scandal_df = pd.read_csv('title_scandal_severity.csv')\n",
    "title_scandal_df['Date'] = pd.to_datetime(title_scandal_df['Date'])\n",
    "title_scandal_df['company_in_title'] = title_scandal_df['company_in_title'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "\n",
    "\n",
    "text_scandal_df = pd.read_csv('text_scandal_severity.csv')\n",
    "text_scandal_df['Date'] = pd.to_datetime(text_scandal_df['Date'])\n",
    "text_scandal_df['company_in_text'] = text_scandal_df['company_in_text'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "\n",
    "\n",
    "all_articles_df = pd.read_csv('final_data_with_count_and_annotation.csv')\n",
    "all_articles_df['Date'] = pd.to_datetime(all_articles_df['Date'])\n",
    "all_articles_df['company_in_title'] = all_articles_df['company_in_title'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "all_articles_df['company_in_text'] = all_articles_df['company_in_text'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "\n",
    "\n",
    "# Filter for Apple-related articles\n",
    "title_enron = title_scandal_df[title_scandal_df['company_in_title'].apply(lambda x: 'Apple' in x)]\n",
    "text_enron = all_articles_df[all_articles_df['company_in_text'].apply(lambda x: 'Apple' in x)]\n",
    "all_enron = all_articles_df[all_articles_df['company_in_title'].apply(lambda x: 'Apple' in x)]\n",
    "\n",
    "# Filter dates\n",
    "start_date = pd.to_datetime('2014-08-01').date()\n",
    "end_date = pd.to_datetime('2014-11-01').date()\n",
    "\n",
    "title_enron['Date'] = title_enron['Date'].dt.date\n",
    "text_enron['Date'] = text_enron['Date'].dt.date\n",
    "all_enron['Date'] = all_enron['Date'].dt.date\n",
    "\n",
    "title_enron = title_enron[(title_enron['Date'] >= start_date) & (title_enron['Date'] <= end_date)]\n",
    "text_enron = text_enron[(text_enron['Date'] >= start_date) & (text_enron['Date'] <= end_date)]\n",
    "all_enron = all_enron[(all_enron['Date'] >= start_date) & (all_enron['Date'] <= end_date)]\n",
    "\n",
    "# Load stock data\n",
    "stock_data = pd.read_excel('../Stock_Final.xlsx', sheet_name=5)\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], errors='coerce').dt.date\n",
    "\n",
    "# Drop rows with invalid dates or missing stock prices\n",
    "stock_data.dropna(subset=['Date', 'PX_VOLUME'], inplace=True)\n",
    "\n",
    "\n",
    "stock_data = stock_data[(stock_data['Date'] >= start_date) & (stock_data['Date'] <= end_date)]\n",
    "\n",
    "# Count the number of articles per day for each dataset\n",
    "articles_per_day_title = title_enron.groupby('Date').size().reset_index(name='Article_Count_Title')\n",
    "articles_per_day_text = text_enron.groupby('Date').size().reset_index(name='Article_Count_Text')\n",
    "articles_per_day_all = all_enron.groupby('Date').size().reset_index(name='Article_Count_All')\n",
    "\n",
    "# Merge the trading volume data with the articles count data\n",
    "merged_df = pd.merge(stock_data, articles_per_day_title, on='Date', how='left').fillna(0)\n",
    "merged_df = pd.merge(merged_df, articles_per_day_text, on='Date', how='left').fillna(0)\n",
    "merged_df = pd.merge(merged_df, articles_per_day_all, on='Date', how='left').fillna(0)\n",
    "\n",
    "\n",
    "def millions(x, pos):\n",
    "    return '%1.0fM' % (x * 1e-6)\n",
    "\n",
    "# Create a formatter for the y-axis\n",
    "formatter = FuncFormatter(millions)\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "\n",
    "ax1.bar(merged_df['Date'], merged_df['PX_VOLUME'], color='#a2cffe', alpha=0.6, label='Trading Volume')\n",
    "ax1.yaxis.set_major_formatter(formatter)  # Apply the formatter to the y-axis\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(merged_df['Date'], merged_df['Article_Count_Title'], color='#ffb347', alpha=0.8, label='Scandal Articles based on Title', linewidth=2)\n",
    "\n",
    "\n",
    "ax2.plot(merged_df['Date'], merged_df['Article_Count_Text'], color='#ff6961', alpha=0.8, label='Scandal Articles based on Text', linewidth=2)\n",
    "\n",
    "\n",
    "ax2.plot(merged_df['Date'], merged_df['Article_Count_All'], color='#c3a284', alpha=0.8, label='All Articles', linewidth=2)\n",
    "\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Trading Volume (Millions)', color='#1f77b4')\n",
    "ax2.set_ylabel('Number of Articles')\n",
    "ax2.set_ylim(bottom=0)  # Ensure y-axis starts at 0\n",
    "plt.title('Apple Trading Volume and Number of Articles Per Day')\n",
    "\n",
    "\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Load the data from the provided CSV file to inspect the first few rows and the data structure\n",
    "file_path = 'scandal_in_title.csv'\n",
    "all_art = pd.read_csv(file_path)\n",
    "all_art.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform list strings into comma-separated words\n",
    "def transform_list_string(column):\n",
    "    return column.apply(lambda x: ', '.join(ast.literal_eval(x)) if pd.notnull(x) else x)\n",
    "\n",
    "# Identify columns to transform\n",
    "columns_to_transform = ['words', 'company_in_title', 'company_in_text', 'scandal_in_title', 'scandal_in_text']\n",
    "\n",
    "# Apply transformation\n",
    "for col in columns_to_transform:\n",
    "    if col in all_art.columns:\n",
    "        all_art[col] = transform_list_string(all_art[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_art['company_in_title'] = all_art['company_in_title'].apply(lambda x: x.split(', ') if isinstance(x, str) else [])\n",
    "data_exploded = all_art.explode('company_in_title')\n",
    "\n",
    "# Count the occurrences of each company\n",
    "company_counts = data_exploded['company_in_title'].value_counts()\n",
    "\n",
    "# Count the occurrences of each company\n",
    "company_counts = data_exploded['company_in_title'].value_counts()\n",
    "\n",
    "# Select the top 15 companies with the most occurrences\n",
    "top_15_companies = company_counts.head(15)\n",
    "\n",
    "# Define shades of blue (reversed to make the most cited one the darkest)\n",
    "blue_shades = [\n",
    "    '#004040',  \n",
    "    '#005560', \n",
    "    '#006C80', \n",
    "    '#0083A0', \n",
    "    '#009AC0',  \n",
    "    '#00B1FF',  \n",
    "    '#1AB9FF',  \n",
    "    '#33C0FF',  \n",
    "    '#4CC8FF', \n",
    "    '#66CFFF',  \n",
    "    '#7FD7FF',  \n",
    "    '#99DFFF',  \n",
    "    '#B2E7FF',  \n",
    "    '#CCEFFF',  \n",
    "    '#E6F7FF'  \n",
    "]\n",
    "\n",
    "\n",
    "colors = blue_shades[:len(top_15_companies)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(top_15_companies, labels=top_15_companies.index, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "plt.title('Top 15 Companies with Most NYT Articles Related to Scandals')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Load the data from the provided CSV file to inspect the first few rows and the data structure\n",
    "file_path = 'scandal_in_title.csv'\n",
    "all_art = pd.read_csv(file_path)\n",
    "all_art.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_art['Year'] = pd.to_datetime(all_art['Date']).dt.year\n",
    "# Create an empty dictionary to store the number of mentions for each company each year\n",
    "yearly_company_counts = {}\n",
    "\n",
    "# Iterate through each row of data\n",
    "for index, row in all_art.iterrows():\n",
    "    year = row['Year']  # Get the year\n",
    "    companies_str = row['company_in_title']  # Get the company list string\n",
    "\n",
    "    # Ensure companies_str is a string\n",
    "    if not isinstance(companies_str, str):\n",
    "        continue\n",
    "\n",
    "    # Use regular expressions to extract company names, i.e., those contained within single quotes and square brackets\n",
    "    companies_list = re.findall(r\"'([^']*)'\", companies_str)\n",
    "    \n",
    "   \n",
    "    for company in companies_list:\n",
    "        \n",
    "        cleaned_company = company.strip().upper()\n",
    "        \n",
    "        if cleaned_company in yearly_company_counts:\n",
    "            \n",
    "            if year in yearly_company_counts[cleaned_company]:\n",
    "                yearly_company_counts[cleaned_company][year] += 1\n",
    "            else:\n",
    "                yearly_company_counts[cleaned_company][year] = 1\n",
    "        else:\n",
    "            yearly_company_counts[cleaned_company] = {year: 1}\n",
    "\n",
    "\n",
    "company_display_names = {\n",
    "    'MICROSOFT': 'Microsoft',\n",
    "    'GOOGLE': 'Google',\n",
    "    'ENRON': 'Enron',\n",
    "    'INTEL': 'Intel',\n",
    "    'FACEBOOK': 'Facebook',\n",
    "    'APPLE': 'Apple',\n",
    "    'CITI': 'Citi',\n",
    "    'BOFA': 'Bank of America',\n",
    "    'AMAZON': 'Amazon',\n",
    "    'BOEING': 'Boeing',\n",
    "    'HEALTHSOUTH': 'HealthSouth'\n",
    "}\n",
    "\n",
    "selected_companies = ['MICROSOFT', 'GOOGLE', 'ENRON', 'INTEL', 'FACEBOOK', 'APPLE', 'CITI', 'BOFA', 'AMAZON', 'BOEING', 'HEALTHSOUTH']\n",
    "\n",
    "df = pd.DataFrame.from_dict(yearly_company_counts, orient='index').fillna(0)\n",
    "\n",
    "# Reorder columns in the desired order of company names\n",
    "df = df.transpose()\n",
    "sorted_df = df.sort_index(axis=0)\n",
    "\n",
    "# Create an empty DataFrame to store the selected columns\n",
    "selected_df = pd.DataFrame()\n",
    "\n",
    "for company in selected_companies:\n",
    "   \n",
    "    if company in sorted_df.columns:\n",
    "        # If it exists, add the column to the selected DataFrame\n",
    "        selected_df[company_display_names[company]] = sorted_df[company]\n",
    "\n",
    "\n",
    "normalized_df = selected_df.div(selected_df.sum(axis=1), axis=0)\n",
    "\n",
    "\n",
    "base_color = '#1f77b4'\n",
    "num_shades = len(selected_companies)\n",
    "colors = [mcolors.to_hex(plt.cm.Blues(i / (num_shades - 1))) for i in range(num_shades)]\n",
    "colors.reverse()  # Reverse the order to have the darkest color at the bottom\n",
    "\n",
    "\n",
    "ax = selected_df.plot(kind='bar', stacked=True, figsize=(12, 7), color=colors)\n",
    "ax.set_ylabel('Number of Mentions')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_title('Annual Count of NYT Articles on Companies with the Most Scandals')\n",
    "plt.legend(title='Companies', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('selected_scandalous_companies_distribution.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting years\n",
    "all_art['Year'] = pd.to_datetime(all_art['Date']).dt.year\n",
    "\n",
    "# Create an empty dictionary to store the number of mentions for each company each year\n",
    "yearly_company_counts = {}\n",
    "\n",
    "# Iterate through each row of data\n",
    "for index, row in all_art.iterrows():\n",
    "    year = row['Year']  # Get the year\n",
    "    companies_str = row['company_in_title']  # Get the company list string\n",
    "\n",
    "    # Ensure companies_str is a string\n",
    "    if not isinstance(companies_str, str):\n",
    "        continue\n",
    "\n",
    "    # Use regular expressions to extract company names, i.e., those contained within single quotes and square brackets\n",
    "    companies_list = re.findall(r\"'([^']*)'\", companies_str)\n",
    "    \n",
    "    # Iterate through each extracted company name\n",
    "    for company in companies_list:\n",
    "        # Remove leading and trailing spaces, and convert to uppercase\n",
    "        cleaned_company = company.strip().upper()\n",
    "        # Check if the company is in yearly_company_counts\n",
    "        if cleaned_company in yearly_company_counts:\n",
    "            # Check if the year already exists in the count for that company\n",
    "            if year in yearly_company_counts[cleaned_company]:\n",
    "                yearly_company_counts[cleaned_company][year] += 1\n",
    "            else:\n",
    "                yearly_company_counts[cleaned_company][year] = 1\n",
    "        else:\n",
    "            yearly_company_counts[cleaned_company] = {year: 1}\n",
    "\n",
    "# Map for displaying proper capitalization in the legend\n",
    "company_display_names = {\n",
    "    'MICROSOFT': 'Microsoft',\n",
    "    'GOOGLE': 'Google',\n",
    "    'ENRON': 'Enron',\n",
    "    'INTEL': 'Intel',\n",
    "    'FACEBOOK': 'Facebook',\n",
    "    'APPLE': 'Apple',\n",
    "    'CITI': 'Citi',\n",
    "    'BOFA': 'Bank of America',\n",
    "    'AMAZON': 'Amazon',\n",
    "    'BOEING': 'Boeing',\n",
    "    'HEALTHSOUTH': 'HealthSouth'\n",
    "}\n",
    "\n",
    "# Select the specific companies of interest\n",
    "selected_companies = ['MICROSOFT', 'GOOGLE', 'ENRON', 'INTEL', 'FACEBOOK', 'APPLE', 'CITI', 'BOFA', 'AMAZON', 'BOEING', 'HEALTHSOUTH']\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame.from_dict(yearly_company_counts, orient='index').fillna(0)\n",
    "\n",
    "# Reorder columns in the desired order of company names\n",
    "df = df.transpose()\n",
    "sorted_df = df.sort_index(axis=0)\n",
    "\n",
    "# Create an empty DataFrame to store the selected columns\n",
    "selected_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through the selected companies list\n",
    "for company in selected_companies:\n",
    "    # Check if the company name column exists in the DataFrame\n",
    "    if company in sorted_df.columns:\n",
    "        # If it exists, add the column to the selected DataFrame\n",
    "        selected_df[company_display_names[company]] = sorted_df[company]\n",
    "\n",
    "# Normalize the data by dividing each company's count by the total count for the selected companies that year\n",
    "normalized_df = selected_df.div(selected_df.sum(axis=1), axis=0)\n",
    "\n",
    "# Generate a gradient of shades from the base color\n",
    "base_color = '#1f77b4'\n",
    "num_shades = len(selected_companies)\n",
    "colors = [mcolors.to_hex(plt.cm.Blues(i / (num_shades - 1))) for i in range(num_shades)]\n",
    "colors.reverse()  # Reverse the order to have the darkest color at the bottom\n",
    "\n",
    "# Plot the stacked bar chart with normalized data and gradient of shades\n",
    "ax = normalized_df.plot(kind='bar', stacked=True, figsize=(12, 7), color=colors)\n",
    "ax.set_ylabel('Percentage of Mentions')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_title('Annual Distribution of WSJ Articles on Companies with the Most Scandals in Percentage')\n",
    "plt.legend(title='Companies', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Set the y-axis labels to be percentages\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x * 100)}%'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('selected_scandalous_companies_distribution_percentage.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Load your dataset (assuming the DataFrame is already loaded as 'all_art')\n",
    "# Make sure the 'Date' and 'company_in_title' columns exist in the DataFrame\n",
    "if 'Date' not in all_art.columns or 'company_in_title' not in all_art.columns:\n",
    "    raise ValueError(\"The DataFrame must contain 'Date' and 'company_in_title' columns\")\n",
    "\n",
    "# Extracting years\n",
    "all_art['Year'] = pd.to_datetime(all_art['Date']).dt.year\n",
    "\n",
    "# Create an empty dictionary to store the number of articles each year\n",
    "yearly_article_counts = {}\n",
    "\n",
    "# Iterate through each row of data\n",
    "for index, row in all_art.iterrows():\n",
    "    year = row['Year']  # Get the year\n",
    "    if year in yearly_article_counts:\n",
    "        yearly_article_counts[year] += 1\n",
    "    else:\n",
    "        yearly_article_counts[year] = 1\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "yearly_article_counts_df = pd.DataFrame(list(yearly_article_counts.items()), columns=['Year', 'Total Articles'])\n",
    "\n",
    "# Sort the DataFrame by year\n",
    "yearly_article_counts_df = yearly_article_counts_df.sort_values(by='Year')\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.bar(yearly_article_counts_df['Year'], yearly_article_counts_df['Total Articles'], color='#1f77b4')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Number of Articles')\n",
    "plt.title('Total Number of WSJ Scandal Articles about Companies Published Per Year')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('total_scandal_articles_per_year.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of top scandal terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"scandal_in_title.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform list strings into comma-separated words - easier to handle\n",
    "def transform_list_string(column):\n",
    "    return column.apply(lambda x: ', '.join(ast.literal_eval(x)) if pd.notnull(x) else x)\n",
    "\n",
    "# Identify columns to transform\n",
    "columns_to_transform = ['words', 'company_in_title', 'company_in_text', 'scandal_in_title', 'scandal_in_text']\n",
    "\n",
    "# Apply transformation\n",
    "for col in columns_to_transform:\n",
    "    if col in data.columns:\n",
    "        data[col] = transform_list_string(data[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' to datetime format and extract the year\n",
    "data['Year'] = pd.to_datetime(data['Date']).dt.year\n",
    "\n",
    "# Handle missing values in 'scandal_in_title' column\n",
    "data['scandal_in_title'] = data['scandal_in_title'].fillna('')\n",
    "\n",
    "# Create a list of scandal words from 'scandal_in_title' column\n",
    "data['scandal_in_title'] = data['scandal_in_title'].apply(lambda x: x.split('__') if x != '' else [])\n",
    "\n",
    "# Explode the dataframe so each scandal term has its own row\n",
    "expanded_data = data.explode('scandal_in_title')\n",
    "\n",
    "# Count the occurrences of each scandal term per year\n",
    "count_per_year = expanded_data.groupby(['Year', 'scandal_in_title']).size().unstack(fill_value=0)\n",
    "\n",
    "# Identify the top 10 scandal words by their total frequency over all years\n",
    "top_scandals = count_per_year.sum(axis=0).nlargest(10).index\n",
    "\n",
    "# Sort the scandal terms by total frequency from highest to lowest\n",
    "sorted_scandals = count_per_year[top_scandals].sum().sort_values(ascending=False).index\n",
    "\n",
    "# Define a base color\n",
    "base_color = '#1f77b4'\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 8))\n",
    "bar_width = 0.8\n",
    "years = count_per_year.index\n",
    "num_scandals = len(sorted_scandals)\n",
    "\n",
    "# Initialize a base for the bar stacking\n",
    "base = np.zeros(len(years))\n",
    "\n",
    "# Loop to plot each scandal term, starting with the most frequent at the bottom\n",
    "for i, scandal in enumerate(sorted_scandals):\n",
    "    # Calculate the color intensity based on position, darker for higher frequency\n",
    "    color_intensity = 1 - (i / num_scandals)  # Invert to have darker colors for more frequent terms\n",
    "    color = plt.cm.Blues(color_intensity)  # Get color from the 'Blues' colormap\n",
    "\n",
    "    # Plot the bar\n",
    "    plt.bar(years, count_per_year[scandal], width=bar_width, label=scandal, color=color, bottom=base)\n",
    "\n",
    "    # Update the base for the next bar\n",
    "    base += count_per_year[scandal]\n",
    "\n",
    "plt.title('Distribution of Top 10 Scandal Terms Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(title='Scandal Terms', loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Frequency Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Take our dictionary\n",
    "keywords = [\n",
    "    \"corrupt.*\", \"deceit.*\", \"decept.*\", \"deceiv.*\", \"betray.*\", \"shame.*\", \n",
    "    \"scandal.*\", \"dishonest.*\", \"misconduct.*\", \"fraud\", \"illegal.*\", \n",
    "    \"unethic.*\", \"violat.*\", \"falsify.*\", \"breach.*\", \"leak.*\", \"pollut.*\", \n",
    "    \"insecur.*\", \"irregular.*\", \"mismanag.*\", \"inappropriat.*\", \"unlaw.*\", \n",
    "    \"transgress.*\", \"noncomplian.*\", \"non-complian.*\", \"ghost.*\", \"malfeas.*\", \n",
    "    \"exploitat.*\", \"discriminat.*\", \"harass.*\", \"misrepresent.*\", \"embezzle.*\", \n",
    "    \"improper.*\", \"espion.*\", \"collus.*\", \"misus.*\", \"rigg.*\", \"kickback.*\", \n",
    "    \"retaliat.*\", \"moral lapse\", \"insider trading\", \"insider dealing\", \n",
    "    \"ponzi scheme\", \"arrest.*\", \"product recall\", \"privacy breach\", \n",
    "    \"privacy violation.*\", \"data leak\", \"intellectual property dispute\", \n",
    "    \"malpracti.*\", \"destruct.*\", \"unsustain.*\", \"ESG scandal\", \"fraudulent\", \n",
    "    \"deceptiv.*\", \"scamm.*\", \"briber.*\", \"bribe.*\", \"extort.*\", \n",
    "    \"misappropriat.*\", \"sabotag.*\", \"deforest.*\", \"habitat destruct.*\", \n",
    "    \"climate change deni.*\", \"tax evasion\", \"money launder.*\", \n",
    "    \"accounting scandal\", \"whistleblow.*\", \"sexual harass.*\", \n",
    "    \"workplace harass.*\", \"toxic culture\", \"data breach\", \"ransomware\", \n",
    "    \"drug recall\", \"clinical trial fraud\", \"off-label marketing\", \"antitrust\", \n",
    "    \"cartel\", \"monopoly\", \"litigat.*\", \"regulatory breach\", \"cover-up\", \n",
    "    \"settlement\", \"lawsuit\", \"penalt.*\"\n",
    "]\n",
    "\n",
    "# Compile the regular expressions for efficiency\n",
    "keyword_patterns = [re.compile(keyword, re.IGNORECASE) for keyword in keywords]\n",
    "\n",
    "# Initialize a counter for the keywords\n",
    "keyword_counter = Counter()\n",
    "\n",
    "# Iterate through the titles and count keyword matches\n",
    "for title in data['Title']:\n",
    "    for pattern in keyword_patterns:\n",
    "        if pattern.search(title):\n",
    "            keyword_counter[pattern.pattern] += 1\n",
    "\n",
    "# Convert the counter to a DataFrame for plotting\n",
    "scandal_df = pd.DataFrame.from_dict(keyword_counter, orient='index', columns=['Count']).reset_index()\n",
    "scandal_df.rename(columns={'index': 'Scandal'}, inplace=True)\n",
    "scandal_df = scandal_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(20, 10))  # Increased figure size for better readability\n",
    "plt.bar(scandal_df['Scandal'], scandal_df['Count'], color='#a2cffe', alpha=0.8)\n",
    "plt.xlabel('Scandal Words', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Most Used Scandal Words in Titles', fontsize=16)\n",
    "plt.xticks(rotation=90, ha='center', fontsize=12)  # Changed rotation and increased font size\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Occurence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"scandal_in_title.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform list strings into comma-separated words - easier to handle --> easier for handling\n",
    "def transform_list_string(column):\n",
    "    return column.apply(lambda x: ', '.join(ast.literal_eval(x)) if pd.notnull(x) else x)\n",
    "\n",
    "\n",
    "columns_to_transform = ['words', 'company_in_title', 'company_in_text', 'scandal_in_title', 'scandal_in_text']\n",
    "\n",
    "\n",
    "for col in columns_to_transform:\n",
    "    if col in data.columns:\n",
    "        data[col] = transform_list_string(data[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "titles = data[['Title', 'company_in_title', 'scandal_in_title']]\n",
    "scandal_titles = titles[titles['scandal_in_title'].notna()]\n",
    "\n",
    "# Parse and tokenize the scandal-related words\n",
    "def tokenize_scandal_words(scandal_str):\n",
    "    return scandal_str.replace('__', ' ').replace(',', ' ').split()\n",
    "\n",
    "scandal_titles['scandal_in_title'] = scandal_titles['scandal_in_title'].apply(lambda x: tokenize_scandal_words(x) if isinstance(x, str) else [])\n",
    "\n",
    "all_scandals = [scandal for sublist in scandal_titles['scandal_in_title'] for scandal in sublist]\n",
    "unique_scandals = list(set(all_scandals))\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=unique_scandals, lowercase=True, binary=True)\n",
    "X = vectorizer.fit_transform(scandal_titles['Title'])\n",
    "\n",
    "\n",
    "company_words = defaultdict(Counter)\n",
    "\n",
    "for index, row in scandal_titles.iterrows():\n",
    "    companies = row['company_in_title'].split(', ')\n",
    "    title_vector = X[index].toarray()[0]\n",
    "    for company in companies:\n",
    "        company_words[company].update(dict(zip(unique_scandals, title_vector)))\n",
    "\n",
    "\n",
    "company_counts = Counter()\n",
    "for companies in scandal_titles['company_in_title']:\n",
    "    for company in companies.split(', '):\n",
    "        company_counts[company] += 1\n",
    "\n",
    "top_companies = [company for company, count in company_counts.most_common(11)]\n",
    "\n",
    "\n",
    "plot_data = []\n",
    "for company in top_companies:\n",
    "    top_words = company_words[company].most_common(11)  # Get top 10 scandal words for each company\n",
    "    for word, freq in top_words:\n",
    "        plot_data.append([company, word, freq])\n",
    "\n",
    "df_plot = pd.DataFrame(plot_data, columns=['Company', 'Word', 'Frequency'])\n",
    "\n",
    "\n",
    "df_plot['Frequency'] = df_plot['Frequency'] / df_plot.groupby('Company')['Frequency'].transform('sum')\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "bubble_plot = sns.scatterplot(data=df_plot, x='Company', y='Word', size='Frequency', hue='Company', legend=False, sizes=(20, 2000), alpha=0.6)\n",
    "\n",
    "plt.title('Top 10 Scandal Words in Titles Associated with Top 10 Companies')\n",
    "plt.xlabel('Company')\n",
    "plt.ylabel('Word')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scandals = pd.read_csv(\"Scandal_in_title.csv\", index_col=0)\n",
    "scandals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoted = pd.read_csv(\"ScandalsRevised.csv\", index_col=0)\n",
    "annoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(scandals, annoted, left_on='Title', right_on='text', how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['Unnamed: 2','Date_y','Comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"ScandalsRevised1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from gensim.models import KeyedVectors\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained Word2Vec model (Google's pre-trained model)\n",
    "word2vec_path = '../GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to preprocess text data and convert to average Word2Vec vectors\n",
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, word2vec, vector_size=300):\n",
    "        self.word2vec = word2vec\n",
    "        self.vector_size = vector_size\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        processed_docs = X.apply(self._preprocess)\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[word] for word in words if word in self.word2vec]\n",
    "                    or [np.zeros(self.vector_size)], axis=0)\n",
    "            for words in processed_docs.str.split()\n",
    "        ])\n",
    "\n",
    "    def _preprocess(self, doc):\n",
    "        doc = re.sub('[^a-zA-Z]', ' ', doc)\n",
    "        doc = doc.lower().split()\n",
    "        doc = [self.lemmatizer.lemmatize(word) for word in doc if word not in self.stop_words]\n",
    "        return ' '.join(doc)\n",
    "\n",
    "# Load the labeled dataset\n",
    "labeled_df = pd.read_csv('ScandalsRevised1.csv')\n",
    "\n",
    "# Preprocess the dataset\n",
    "labeled_df = labeled_df[[\"GOID\", \"text\", \"label\"]]\n",
    "labeled_df['label'] = labeled_df['label'].fillna('N')\n",
    "\n",
    "X = labeled_df['text']\n",
    "y = labeled_df['label']\n",
    "\n",
    "# Ensure all values in X are strings\n",
    "X = X.astype(str)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.31, random_state=42)\n",
    "\n",
    "# Transform the text data to Word2Vec vectors\n",
    "word2vec_transformer = Word2VecTransformer(word2vec)\n",
    "X_train_vectors = word2vec_transformer.transform(X_train)\n",
    "X_test_vectors = word2vec_transformer.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to the numerical vectors to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectors, y_train)\n",
    "\n",
    "# Define pipeline for the Logistic Regression classifier\n",
    "PipeLR = Pipeline([\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Train the model on the resampled training data\n",
    "PipeLR.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "PredictLR = PipeLR.predict(X_test_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print accuracy scores and classification reports\n",
    "print(\"LogisticRegression Accuracy:\", accuracy_score(y_test, PredictLR))\n",
    "print(\"Classification Report for LogisticRegression:\")\n",
    "print(classification_report(y_test, PredictLR))\n",
    "\n",
    "print(\"Confusion Matrix for LogisticRegression:\")\n",
    "cm_lr = confusion_matrix(y_test, PredictLR)\n",
    "print(cm_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the confusion matrix for LogisticRegression\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', xticklabels=['N', 'Y'], yticklabels=['N', 'Y'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for LogisticRegression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
